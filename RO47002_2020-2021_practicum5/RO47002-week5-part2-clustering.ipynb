{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RO47002 Machine Learning for Robotics\n",
    "* (c) TU Delft, 2020\n",
    "* Period: 2020-2021, Q1\n",
    "* Course homepage: https://brightspace.tudelft.nl/d2l/home/318952"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE`. Moreover, if you see an empty cell, please DO NOT delete it, instead run that cell as you would run all other cells. Please fill in your name(s) and other required details below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_NUMBER = \"\"\n",
    "STUDENT_NAME1 = \"\"\n",
    "STUDENT_NUMBER1 = \"\"\n",
    "STUDENT_NAME2 = \"\"\n",
    "STUDENT_NUMBER2 = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3f76d6a626db81c484191482b101edb",
     "grade": true,
     "grade_id": "cell-c35e4c8223095209",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Note: this block is a check that you have filled in the above information.\n",
    "# It will throw an AssertionError until all fields are filled\n",
    "assert(GROUP_NUMBER != \"\")\n",
    "assert(STUDENT_NAME1 != \"\")\n",
    "assert(STUDENT_NUMBER1 != \"\")\n",
    "assert(STUDENT_NAME2 != \"\")\n",
    "assert(STUDENT_NUMBER2 != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General announcements\n",
    "\n",
    "* Do *not* share your solutions, and do *not* copy solutions from others. By submitting your solutions, you claim that you and your lab partner alone are responsible for this code.\n",
    "\n",
    "* Do *not* email questions directly, since we want to provide everybody with the same information and avoid repeating the same answers. Instead, please post your questions regarding this assignment in the correct support forum on Brightspace, this way everybody can benefit from the response. If you do have a particular question that you want to ask directly, please use the scheduled practicum hours to ask a TA.\n",
    "\n",
    "* There is a strict deadline for each assignment. Students are responsible to ensure that they have uploaded their work in time. So, please double check that your upload succeeded to the Brightspace and avoid any late penalties.\n",
    "\n",
    "* This [Jupyter notebook](https://jupyter.org/) uses `nbgrader` to help us with automated tests. `nbgrader` will make various cells in this notebook \"uneditable\" or \"unremovable\" and gives them a special id in the cell metadata. This way, when we run our checks, the system will check the existence of the cell ids and verify the number of points and which checks must be run. While there are ways that you can edit the metadata and work around the restrictions to delete or modify these special cells, you should not do that since then our nbgrader backend will not be able to parse your notebook and give you points for the assignment. You are free to add additional cells, but if you find a cell that you cannot modify or remove, please know that this is on purpose.\n",
    "\n",
    "* This notebook will have in various places a line that throws a `NotImplementedError` exception. These are locations where the assignment requires you to adapt the code! These lines are just there as a reminder for youthat you have not yet adapted that particular piece of code, especially when you execute all the cells. Once your solution code replaced these lines, it should accordingly *not* throw any exceptions anymore.\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# week 5\n",
    "\n",
    "* Topic: Dimensionality reduction, clustering\n",
    "* Year: 2020-2021\n",
    "* Book chapters: 8, 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This lab assignment consists of two parts.\n",
    "\n",
    "**This is Part 2 - Clustering**\n",
    "\n",
    "In this part you will:\n",
    "\n",
    "* Using K-Means on a toy dataset\n",
    "* Implementing the basic K-Means algorithm yourself\n",
    "* Using a Gaussian Mixture Model (GMM) for clustering\n",
    "* Comparing K-Means and GMM for outlier detection\n",
    "* Application: *Meeting and recognizing human faces*\n",
    "* A simple semi-supervised approach to improve clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "#%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "We start again by exploring some clustering techinques on a toy dataset,\n",
    "which is generated here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Create toy dataset\n",
    "n_samples = 1000\n",
    "\n",
    "X, y = make_blobs(n_samples=n_samples,\n",
    "    cluster_std=[0.5, 2.0, 1.5],\n",
    "    random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show plot of the data\n",
    "def plot_data_with_labels(X, y=None):\n",
    "    s = plt.scatter(X[:,0], X[:,1], c=y)\n",
    "    if not y is None:\n",
    "        plt.legend(*s.legend_elements())\n",
    "        \n",
    "plot_data_with_labels(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common algorithm used to cluster data is **k-means**.\n",
    "In the block below,\n",
    "\n",
    "* use sklearn's k-Means implementation to cluster the given data `X`, and predict each sample's cluster label. Initialize k-Means by assuming that there are $k=3$ clusters in the data.\n",
    "* also obtain the predictions of the fitted k-Means solutions on all samples in `X`, and call this `y_pred_km`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7e64405ee88fa25c9690623ea15f323",
     "grade": false,
     "grade_id": "cell-c858924165ff1e40",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "kmeans = None # store sklearns k-Means implementation here\n",
    "y_pred_km = None # store the predicted cluster labels for data X here \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfa011e2c576bdb42e3e28e002edd4b7",
     "grade": true,
     "grade_id": "cell-b1698a40756eb52e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert( kmeans != None )\n",
    "assert( len(np.unique(y_pred_km)) == 3 )\n",
    "assert( y_pred_km.shape == (n_samples,) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's checkout the cluster labels that the algorithm found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color the samples with the predicted labels\n",
    "plot_data_with_labels(X, y_pred_km)\n",
    "\n",
    "# also show the cluster centers found by k-Means\n",
    "plt.plot(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], 'rd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could fit a Gaussian Mixture distribution on the data,\n",
    "and label the samples based on their probability under the fitted mixture components.\n",
    "\n",
    "In the code below, fit a Gaussian Mixture on the data using $k=3$ mixture components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8963ce359d385e98cfa3dadabe1ede25",
     "grade": false,
     "grade_id": "cell-3c71cdcaf943a55c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "gm = None # store your sklearn GaussianMixture here\n",
    "y_pred_gm = None # store the predicted class labels from the GaussianMixture here\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ab4e7a2e7a3f3398749dfbe80eca3b0",
     "grade": true,
     "grade_id": "cell-e1e3b25f849405be",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert( gm != None )\n",
    "assert( len(np.unique(y_pred_gm)) == 3 )\n",
    "assert( y_pred_gm.shape == (n_samples,) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again visualize the result, and compare it to k-Means.\n",
    "You should see that the plots look pretty similar, except perhaps for a few samples near the cluster boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], 'rd')\n",
    "plot_data_with_labels(X, y_pred_km)\n",
    "plt.title('k-Means')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(gm.means_[:,0], gm.means_[:,1], 'rd')\n",
    "plot_data_with_labels(X, y_pred_gm)\n",
    "plt.title('Gaussian Mixture')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement k-means algorithm\n",
    "\n",
    "The basic k-Means algorithm actually consists only a few steps:\n",
    "\n",
    "1. initialize k cluster centers by picking k random samples\n",
    "2. assign samples to nearest cluster center\n",
    "3. for each cluster k, compute the new center as the mean of the assigned samples\n",
    "4. goto step 2, until convergence or sufficient iterations have passed\n",
    "\n",
    "In the code blocks below, you will implement steps 1 to 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement **step 1**: select k samples as random samples from X and set these as cluster centers.\n",
    "*HINT:* you can use `np.random.choice` to randomly select k indices from a given range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29564b46c73f0eb9bfd59793a44e4aef",
     "grade": false,
     "grade_id": "cell-b8cd2167361538be",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b809a9df06665a3dfecc1d6801c577df",
     "grade": true,
     "grade_id": "cell-15ba7d96be57b058",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "np.random.seed(5)\n",
    "centers = initialize_centers(k, X)\n",
    "\n",
    "assert(centers.shape == (k, 2)) # k centers in a 2D feature space\n",
    "\n",
    "centers_init = centers.copy() # keep a copy to compare later to\n",
    "\n",
    "# let's plot the randomly selected cluster centers\n",
    "plot_data_with_labels(X)\n",
    "plt.plot(centers[:,0], centers[:,1], 'rd')\n",
    "plt.title('Initial cluster centers (no data assigned yet)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement **step 2**: assign all data points to nearest centers (in terms of Euclidean distance)\n",
    "*Hint:* you can use `scipy.spatial.distance.cdist` to quickly compute the distance of the samples to the cluster centers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04891792a18a60c378b035a1ec00a78c",
     "grade": false,
     "grade_id": "cell-08c553fc7f5d3ec4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def assign_samples_to_centers(centers, X):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return cluster_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f9ad847e740ed802942b22ee9712bdd",
     "grade": true,
     "grade_id": "cell-167bfcea73922c65",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "cluster_idxs = assign_samples_to_centers(centers, X)\n",
    "\n",
    "assert( cluster_idxs.shape == (n_samples,) )\n",
    "assert(cluster_idxs.min() >= 0) # lowest possible cluster id is 0\n",
    "assert(cluster_idxs.max() < k) # highest possible cluster id is k-1\n",
    "\n",
    "# Let's see how the samples were assigned to the randomly picked cluster centers\n",
    "plt.plot(centers[:,0], centers[:,1], 'rd')\n",
    "plot_data_with_labels(X, cluster_idxs)\n",
    "plt.title('Assigning data to nearest cluster centers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement **step 3**: compute new center means based on the sasigned cluster labels.\n",
    "In this step, iterate over the k cluster labels, and compute the center of cluster $c$ as the mean of all samples in X assigned to $c$ in `cluster_idxs`.\n",
    "The result is a new set of $k$ cluster centers, based on the current assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d842c435420eb2f7677f10a0a2ed4c8",
     "grade": false,
     "grade_id": "cell-2595920badf8056e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def update_center_means(k, X, cluster_idxs):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f6b90d7fdd71a55338800745cdf82d7",
     "grade": true,
     "grade_id": "cell-84d75b6d06f813a9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "centers = update_center_means(k, X, cluster_idxs)\n",
    "\n",
    "assert(centers.shape == (k, 2)) # k centers in a 2D feature space\n",
    "\n",
    "# check that the centers have changed w.r.t. the initial centers that we stored in center_init above\n",
    "assert(np.all( centers_init != centers ))\n",
    "\n",
    "# Show the new assignment\n",
    "plt.plot(centers[:,0], centers[:,1], 'rd')\n",
    "plot_data_with_labels(X, cluster_idxs)\n",
    "plt.title('After recomputing the cluster centers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! If you have implemented these steps correctly, we can run all step sequentially for a few iterations.\n",
    "Notice how after a few iterations already the cluster centers are not moving much anymore.\n",
    "Once we detect that the number of samples assigned to each cluster haven't changed, we can quit iterating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "# Step 1\n",
    "centers = initialize_centers(k, X)\n",
    "\n",
    "plt.plot(centers[:,0], centers[:,1], 'rd')\n",
    "plot_data_with_labels(X)\n",
    "plt.title('initialization')\n",
    "plt.show()\n",
    "\n",
    "max_iters = 10\n",
    "last_cluster_counts = np.zeros(k)\n",
    "for j in range(max_iters):\n",
    "    # Step 2\n",
    "    cluster_idxs = assign_samples_to_centers(centers, X)\n",
    "    \n",
    "    # Step 3\n",
    "    centers = update_center_means(k, X, cluster_idxs)\n",
    "\n",
    "    # Show intermediate result\n",
    "    cluster_counts = np.bincount(cluster_idxs, minlength=k)\n",
    "    plt.title(f'iter {j}: ' + str(cluster_counts))\n",
    "    plt.plot(centers[:,0], centers[:,1], 'rd')\n",
    "    plot_data_with_labels(X, cluster_idxs)\n",
    "    plt.show()\n",
    "    \n",
    "    # Step 4: continue with next iteration, unless converged\n",
    "    if np.all(cluster_counts == last_cluster_counts):\n",
    "        # detected cluster assignment didn't change anymore since last iteration,\n",
    "        # so cluster centers have converged now\n",
    "        print(f'Converged in {j} iterations!')\n",
    "        break # quit the for-loop\n",
    "    else:\n",
    "        # keep this iteration's cluster counters to compare to in next iteration\n",
    "        last_cluster_counts = cluster_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the outliers\n",
    "\n",
    "Let's try to do some anomaly detection, that is finding outliers which are dissimilar to most of the rest of the taining data. One approach one might come up with is to consider any sample \"sufficiently\" far away from a cluster center as an outlier. With k-Means we can reuse the scoring metric used to assign samples to cluster centers, thus a sample is considered an outlier if its Euclidean distance to nearest cluster center is above a certain distance threshold.\n",
    "\n",
    "Complete the function `find_kmeans_distance_outliers()` below to implement this outlier detection method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d5551b3c4919d32f544a8dcb73a36ac",
     "grade": false,
     "grade_id": "cell-21916fcd16d7f444",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_kmeans_distance_outliers(kmeans, X, dist_thresh):\n",
    "    \"\"\"\n",
    "    Determine for each sample in X if it is an outlier or not.\n",
    "    Outliers are found by finding the distance to the closest cluster center.\n",
    "    If this smallest distance is above the given threshold `dist_thresh`,\n",
    "    then the sample is an outlier\n",
    "    \n",
    "    Input: kmeans - a fitted instance of sklearn's KMeans class\n",
    "    Input: X - a N x M dataset of N samples with M features\n",
    "    Input: dist_thresh - a number, outliers are more distance than this to all cluster centers\n",
    "    Returns: is_outlier - a N-dimensional Boolean numpy vector,\n",
    "             is_outlier[i] is True only if sample X[i] is an outlier.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return is_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dce16d79e99195a18b32c6201d7a2117",
     "grade": true,
     "grade_id": "cell-3b424ecaeb1ad495",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "is_outlier = find_kmeans_distance_outliers(kmeans, X, dist_thresh=2.0)\n",
    "assert(len(is_outlier) == n_samples)\n",
    "assert(is_outlier.dtype == np.bool)\n",
    "\n",
    "assert(is_outlier[0] == False) # this sample should not be an outlier for this threshold\n",
    "assert(is_outlier[2] == True) # this sample should be an outlier for this threshold\n",
    "assert(is_outlier[3] == True) # this sample should be an outlier for this threshold\n",
    "assert((sum(is_outlier) > 320) and (sum(is_outlier) < 350)) # total number of expected outliers ~332\n",
    "\n",
    "is_outlier = find_kmeans_distance_outliers(kmeans, X, dist_thresh=5.0)\n",
    "assert(is_outlier[0] == False) # this sample should not be an outlier for this threshold\n",
    "assert(is_outlier[2] == True) # this sample should be an outlier for this threshold\n",
    "assert(is_outlier[3] == False) # this sample should not be an outlier for this threshold\n",
    "assert((sum(is_outlier) > 2) and (sum(is_outlier) < 8)) # total number of expected outliers ~5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive widget to explore outliers\n",
    "def show_kmeans_outliers(dist_thresh):\n",
    "    is_outlier = find_kmeans_distance_outliers(kmeans, X, dist_thresh)\n",
    "    \n",
    "    plt.plot(X[~is_outlier,0], X[~is_outlier,1], '.', label='inlier')\n",
    "    plt.plot(X[is_outlier,0], X[is_outlier,1], '.', label='outlier')\n",
    "    plt.plot(centers[:,0], centers[:,1], 'rd')\n",
    "    plt.legend()\n",
    "    plt.grid('on')\n",
    "\n",
    "ipywidgets.interact(show_kmeans_outliers, dist_thresh=(0., 5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, a better way could be to use a density estimation technique, such as a Gaussian Mixture Model, as seen in the book. To use the Gaussian Mixture for outlier detection, we could now score the samples with respect to the distribution fit on all the data,  i.e. we compute the log probability density of each sample under the distribution.\n",
    "An outlier is than a sample for which the log probability is under a given threshold\n",
    "\n",
    "As you did for k-Means outlier detection, complete the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f5eb9b2903a3f4fdc2a926804d19bbe",
     "grade": false,
     "grade_id": "cell-c90fa1f7675f118f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_gm_logporb_outliers(gm, X, logprob_thresh):\n",
    "    \"\"\"\n",
    "    Determine for each sample in X if it is an outlier or not.\n",
    "    Outliers are found by finding the log-probability under the fitted Gaussian mixture.\n",
    "    If this log probability is BELOW the given threshold `logprob_thresh`,\n",
    "    then the sample is an outlier.\n",
    "    \n",
    "    Input: gm - a fitted instance of sklearn's GaussianMixture class\n",
    "    Input: X - a N x M dataset of N samples with M features\n",
    "    Input: logprob_thresh - a number, outliers how a log-probability under the GM below this threshold\n",
    "    Returns: is_outlier - a N-dimensional Boolean numpy vector,\n",
    "             is_outlier[i] is True only if sample X[i] is an outlier.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return is_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef161296bdd59a7f6cc291d483061f09",
     "grade": true,
     "grade_id": "cell-ba29f37e6837d6f7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "is_outlier = find_gm_logporb_outliers(gm, X, logprob_thresh=-7.0)\n",
    "assert(len(is_outlier) == n_samples)\n",
    "assert(is_outlier.dtype == np.bool)\n",
    "\n",
    "assert(is_outlier[0] == False) # this sample should not be an outlier for this threshold\n",
    "assert(is_outlier[2] == True) # this sample should be an outlier for this threshold\n",
    "assert(is_outlier[3] == False) # this sample should be an outlier for this threshold\n",
    "assert((sum(is_outlier) > 22) and (sum(is_outlier) < 32)) # total number of expected outliers ~27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of log probabilities for our dataset, and see if we can get an idea in what range we should put our cutoff threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(gm.score_samples(X));\n",
    "plt.xlabel('log probability')\n",
    "plt.ylabel('number of samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most samples have a log probability above approximately -7.\n",
    "A cutoff threshold in the range -8 and -6 is probably good, we could reasonably argue that the samples in the tail of this distribution are outliers. Of course in practice the cutoff would be depend on your task, so this is not a general rule.\n",
    "\n",
    "We can study the effect of different thresholds in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive widget to explore outliers\n",
    "def show_gm_outliers(logprob_thresh):\n",
    "    is_outlier = find_gm_logporb_outliers(gm, X, logprob_thresh)\n",
    "    \n",
    "    plt.plot(X[~is_outlier,0], X[~is_outlier,1], '.', label='inlier')\n",
    "    plt.plot(X[is_outlier,0], X[is_outlier,1], '.', label='outlier')\n",
    "    plt.plot(gm.means_[:,0], gm.means_[:,1], 'rd')\n",
    "    plt.legend()\n",
    "    plt.grid('on')\n",
    "\n",
    "ipywidgets.interact(show_gm_outliers, logprob_thresh=(-12., .0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the earlier outlier detection method using the Euclidean distance to the cluster center to this approach of using the log probability under the GMM.\n",
    "\n",
    "**Q**: What happens when you try to detect possible outliers in the smaller cluster in the bottom-right? Explain what you observe with the Euclidean-distance based detection method, and what you observe with the Gaussian Mixture detection method. What causes these differences?\n",
    "\n",
    "*Hint*: think about how the distance between the a sample and the center (mean) are used in the formula for the Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6a6485d35373308f993f2fbf12a439b",
     "grade": true,
     "grade_id": "cell-42c2c9f88f758989",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end this part with a note.\n",
    "\n",
    "An even better way to find outliers with respect to the overall data, would be to fit the distribution on all data EXCEPT the sample we are scoring,\n",
    "since we our now 'testing' the distribution on a sample in the 'training' data.\n",
    "This is what the book refers to *novelty detection*, but an outlier could be consider a novelty w.r.t. the rest of the training data.\n",
    "\n",
    "However, since in these examples we are using few mixture components the overall distributions should not change too much\n",
    "of we keep the test sample included (bias-variance trade-off) and it is much faster to fit the distribution\n",
    "once and test all N samples, instead of fitting a distribution N times for all test samples separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering faces\n",
    "\n",
    "For the final part, let's consider a social robot that moves around a building,\n",
    "and continously bumps into people. Using a trained object detector,\n",
    "it can detect and locate faces in its camera image.\n",
    "Using the bounding box, a square patch around a detected face could be extracted.\n",
    "Over time, the robot would build a dataset of faces, some belonging to the same people and others not.\n",
    "Can you help the robot determine which faces probably correspond to the same person?\n",
    "\n",
    "![Pepper, the social robot (image source: Wikipedia)](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a1/SoftBank_pepper.JPG/330px-SoftBank_pepper.JPG)\n",
    "\n",
    "\n",
    "For this exercise we will use the `Olivetti faces dataset`, containing 400 data samples, with 10 samples for 40 persons.\n",
    "Each sample is a $64 \\times 64$ gray scale image patch, reshaped into a 4096-dimensional feature vector.\n",
    "The class labels of this dataset represent the person identitiy, so all samples with the same class label are taken from the same person.\n",
    "We will pretend that these are the (gray scale) face image patches that our robot has collected.\n",
    "The dataset actually contains a set of face images taken between April 1992 and \n",
    "April 1994 at AT&T Laboratories Cambridge. \n",
    "\n",
    "\n",
    "Note that the first time you run the code below, sklearn will automatically downloads the data\n",
    "archive from AT&T. The result will be cached, so once it is on your computer it will not need to download it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "\n",
    "ALLOW_DOWNLOAD = True\n",
    "\n",
    "#data = sklearn.datasets.fetch_lfw_people(min_faces_per_person=70, resize=0.4, download_if_missing=ALLOW_DOWNLOAD)\n",
    "data = sklearn.datasets.fetch_olivetti_faces(download_if_missing=ALLOW_DOWNLOAD)\n",
    "\n",
    "# To see more information on the downloaded dataset, execute the following line:\n",
    "#print(data['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data['data'], data['target']\n",
    "\n",
    "print('Dataset size:', X.shape)\n",
    "print('Unique person ids:', np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the contents of this dataset, we can explore the features in this dataset by resizing them to $64 \\times 64$ images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_face(idx):\n",
    "    x = X[idx]\n",
    "    x = x.reshape((64,64))\n",
    "    label = y[idx]\n",
    "    plt.imshow(x, cmap='gray')\n",
    "    plt.title(f'sample={idx}, label={label}')\n",
    "    plt.axis('off')\n",
    "\n",
    "ipywidgets.interact(show_face, idx=(0,399))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block below, compare 3 methods of dimensionality reduction to visualize the data in the 4096-dimensional space in a 2D plot, namely:\n",
    "\n",
    "1. Principal Component Analysis (see Part 1 of the exercises)\n",
    "2. Linear Discriminant Analysis (see Part 1 of the exercises)\n",
    "3. t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "Check the sklearn documentation on [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html?highlight=tsne#sklearn.manifold.TSNE) if you are not sure how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09715c7dcac84f7e541882592f48b4ee",
     "grade": false,
     "grade_id": "cell-d14a6c7642c6e8b5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# set these variables\n",
    "X_pca = None # result of dim. reduction with PCA\n",
    "X_lda = None # result of dim. reduction with LDA\n",
    "X_tsne = None # result of dim. reduction with t-SNE\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b53ee72737eebaab1d98d1d8dc18abe",
     "grade": true,
     "grade_id": "cell-5b4fbed90cd8d36e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(X_pca.shape == (400, 2))\n",
    "assert(X_lda.shape == (400, 2))\n",
    "assert(X_tsne.shape == (400, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we can visualize the data in the projected space.\n",
    "We will also in color the points in all plots using the true class labels, such that we can see if the resulting embedding has placed points from the same class together or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2D_embedded_classes(X, y):\n",
    "    for c in np.unique(y):\n",
    "        plt.plot(X[y==c,0], X[y==c,1], '.', label='%d'%c)\n",
    "    plt.grid()\n",
    "    plt.axis('equal')\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.subplot(1,3,1)\n",
    "plot_2D_embedded_classes(X_pca, y)\n",
    "plt.title('PCA')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plot_2D_embedded_classes(X_lda, y)\n",
    "plt.title('LDA')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plot_2D_embedded_classes(X_tsne, y)\n",
    "plt.title('TSNE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Which of these 3 dimensionality reduction techniques appear deterministic, and which stochastic (i.e. provide different results every time you run it?)\n",
    "\n",
    "**Q** Which of these 3 dimensionality reduction techniques are unsupervised, and thus 'blind' to the true class labels (until) we plot them?\n",
    "\n",
    "**Q** Which of these 3 dimensionality reduction techniques compute non-linear projections?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f189dcb2567860ee57aa641cfaa90cae",
     "grade": false,
     "grade_id": "cell-d68b2299995068a3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# To answer, set each variable to either True or False\n",
    "\n",
    "#Q1\n",
    "PCA_IS_DETERMINISTIC = None\n",
    "LDA_IS_DETERMINISTIC = None\n",
    "TSNE_IS_DETERMINISTIC = None\n",
    "\n",
    "#Q2\n",
    "PCA_IS_UNSUPERVISED = None\n",
    "LDA_IS_UNSUPERVISED = None\n",
    "TSNE_IS_UNSUPERVISED = None\n",
    "\n",
    "#Q3\n",
    "PCA_IS_NONLINEAR_PROJECTION = None\n",
    "LDA_IS_NONLINEAR_PROJECTION = None\n",
    "TSNE_IS_NONLINEAR_PROJECTION = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36b9643027c87401f5f316012cf7ec83",
     "grade": true,
     "grade_id": "cell-89f6aaf615f8e477",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(PCA_IS_DETERMINISTIC in (True, False))\n",
    "assert(LDA_IS_DETERMINISTIC in (True, False))\n",
    "assert(TSNE_IS_DETERMINISTIC in (True, False))\n",
    "\n",
    "assert(PCA_IS_UNSUPERVISED in (True, False))\n",
    "assert(LDA_IS_UNSUPERVISED in (True, False))\n",
    "assert(TSNE_IS_UNSUPERVISED in (True, False))\n",
    "\n",
    "assert(PCA_IS_NONLINEAR_PROJECTION in (True, False))\n",
    "assert(LDA_IS_NONLINEAR_PROJECTION in (True, False))\n",
    "assert(TSNE_IS_NONLINEAR_PROJECTION in (True, False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing k-Means and Gaussian Mixture model\n",
    "\n",
    "Ok, now let's see what happens if we try to recover the 40 identities by clustering this data.\n",
    "\n",
    "We do need a metric to evaluate success though.\n",
    "There are many metrics in sklearn to evaluate the cluster quality with respect to some ground truth labels.\n",
    "Here, we will use `sklearn.metrics.completeness_score()`.\n",
    "\n",
    "From the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score):\n",
    "```\n",
    "Completeness metric of a cluster labeling given a ground truth.\n",
    "\n",
    "A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster.\n",
    "\n",
    "This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won’t change the score value in any way.\n",
    "```\n",
    "\n",
    "Overall, this metric will be score between 0.0 for bad (all the clusters are randomly divided over the true class labels) to 1.0 for good (clusters perfectly align with class labels, up to permutation of which class is which cluster)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try to cluster the data into 40 clusters using\n",
    "\n",
    "- k-Means, and\n",
    "- a Gaussian Mixture model.\n",
    "\n",
    "**WARNING** Gaussian component has a $M \\times M$ covariance matrix, which results in a large amount of parameters to be estimated as the number of feature increases (it grows quadratically!).\n",
    "Fitting a mixture model on such high dimensional feature space can therefore take a *loooooong* time before it converges, and will also be very prone to overfitting!\n",
    "\n",
    "**Hint:** First apply PCA to reduce the feature space to, say, 50 components before fitting the Gaussian Mixture.\n",
    "\n",
    "Remember that to debug your code, and you can use `%%time` to figure out how long it takes to run a block on your PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24b84c68009de5a598794aecf6a36e17",
     "grade": false,
     "grade_id": "cell-1cb6992161033640",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "kmeans = None # store your KMeans instance in this variable\n",
    "y_pred_km = None # store your predictions in this variable\n",
    "\n",
    "k = 40\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54e198753ca9784cc34b664bc921e739",
     "grade": false,
     "grade_id": "cell-d35933b9a3e81c1f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gm = None # store your GaussianMixture in this variable\n",
    "y_pred_gm = None # store your predictions in this variable\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ec2208c57b69eeec208fa4cd5fcdd15",
     "grade": true,
     "grade_id": "cell-62cea27bfac0cd92",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "score_km = sklearn.metrics.completeness_score(y, y_pred_km)\n",
    "print('         k-Means:', score_km)\n",
    "\n",
    "score_gm = sklearn.metrics.completeness_score(y, y_pred_gm)\n",
    "print('Gaussian Mixture:', score_gm)\n",
    "\n",
    "assert( kmeans != None )\n",
    "assert( len(np.unique(y_pred_km)) == 40 )\n",
    "assert( y_pred_km.shape == (400,) )\n",
    "\n",
    "assert( gm != None )\n",
    "assert( len(np.unique(y_pred_gm)) == 40 )\n",
    "assert( y_pred_gm.shape == (400,) )\n",
    "\n",
    "sklearn.utils.validation.check_is_fitted(kmeans) # will throw error if not fitted\n",
    "sklearn.utils.validation.check_is_fitted(gm) # will throw error if not fitted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you might see some variance in the results everytime you rerun it.\n",
    "\n",
    "Answer in the block below the following two questions.\n",
    "\n",
    "* **Q1**: can you conclude which clustering method finds better clusters on this dataset?\n",
    "* **Q2**: which approach clusters the data more efficiently in terms of processing time, and is this affected by dimensionality reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b5414919f6986d8644b8f40848aa5f0",
     "grade": true,
     "grade_id": "cell-856cc8fa825cf514",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, let's visually inspect the clusters found by both methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_faces_class(y_pred, c):\n",
    "    idxs = np.where(y_pred == c)[0]\n",
    "\n",
    "    plt.figure(figsize=(16,8))\n",
    "    for j, idx in enumerate(idxs[:15]):\n",
    "        plt.subplot(3,5,j+1)\n",
    "        show_face(idx)\n",
    "\n",
    "print('K-Means')\n",
    "ipywidgets.interact(lambda cluster: plot_faces_class(y_pred_km, cluster), cluster=(0,40-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gaussian Mixture')\n",
    "ipywidgets.interact(lambda cluster: plot_faces_class(y_pred_gm, cluster), cluster=(0,40-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-supervised learning\n",
    "\n",
    "For the final test, let's assume we DO have a few labels, but not all labels.\n",
    "In fact, let's assume that our robot gets one face with a person id for each person,\n",
    "but also collects 9 unlablled pictures of each person.\n",
    "Can we improve our clustering by using the few labelled samples?\n",
    "\n",
    "This would be a case of *semi-supervised* learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's assume that this part of the data is labelled\n",
    "# every 10th sample is a new person\n",
    "X_labelled = X[::10,:]\n",
    "y_labelled = y[::10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline, let's see how a standard Random Forest classifier would perform when given just this 1-sample per class dataset on the full unlabelled dataset.\n",
    "\n",
    "**NOTE**: a better way to draw sound conclusions would be to do a proper train-and-test split and hyperparameter tuning, but a Random Forest is a relatively robust classifier with few hyperparameters, so it is sufficient for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "rf = sklearn.ensemble.RandomForestClassifier()\n",
    "rf.fit(X_labelled, y_labelled)\n",
    "y_pred_rf = rf.predict(X)\n",
    "\n",
    "comp_score = sklearn.metrics.completeness_score(y, y_pred_rf)\n",
    "acc_score = sklearn.metrics.accuracy_score(y, y_pred_rf)\n",
    "\n",
    "print('RF completeness:', comp_score)\n",
    "print('    RF accuracy:', acc_score)\n",
    "\n",
    "print('Confusion matrix')\n",
    "plt.matshow(sklearn.metrics.confusion_matrix(y_pred_rf, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random Forst (supervised on 10 samples)')\n",
    "ipywidgets.interact(lambda cluster: plot_faces_class(y_pred_rf, cluster), cluster=(0,40-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The supervised accuracy of the RF is not great ... but it also was given very little data (1 sample per class!)\n",
    "\n",
    "Let's try to use a GaussianMixture here again, but let's give it a semi-supervised twist:\n",
    "**use the given labelled samples as initial mean estimates of the 40 Gaussian terms**.\n",
    "\n",
    "To do this, take care of the following points:\n",
    "\n",
    "- You will need to use PCA for dimensionality reduction again, try using about 18 components this time\n",
    "- PCA should be fitted on all data, but you need to project the labelled data to the PCA space again separately to define the means of the GaussianMixture in this reduced space\n",
    "- To initialize the means of the GaussianMixture with specific values, you need to set the `init_params` parametere to `'random'`, and then use the `means_init` parameter to define the means. See the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb0e9c04f795e5126e3686a311f4569a",
     "grade": false,
     "grade_id": "cell-9d42c608723ea944",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "gm2 = None # store your semi-supervised GaussianMixture in this variable\n",
    "y_pred_gm2 = None # store the predictions of the semi-supervised GaussianMixture in this variable\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dff355231b388f15392f8143e2a13bda",
     "grade": true,
     "grade_id": "cell-2df81c72fddd435f",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "comp_score_gm2 = sklearn.metrics.completeness_score(y, y_pred_gm2)\n",
    "acc_score_gm2 = sklearn.metrics.accuracy_score(y, y_pred_gm2)\n",
    "\n",
    "print('GM semi-supervised completeness:', comp_score_gm2)\n",
    "print('   GM semi-supervised  accuracy:', acc_score_gm2)\n",
    "\n",
    "# these scores should be achievable (though there is a bit of randomness ...)\n",
    "assert(comp_score_gm2 > 0.82)\n",
    "assert(acc_score_gm2 > 0.75)\n",
    "\n",
    "assert( gm2 != None )\n",
    "assert( len(np.unique(y_pred_gm2)) == 40 )\n",
    "assert( y_pred_gm2.shape == (400,) )\n",
    "\n",
    "sklearn.utils.validation.check_is_fitted(gm2) # will throw error if not fitted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is correct, you should see that semi-supervised training with the GaussianMixture outperforms the fully unsupervised Gaussian Mixture, but also the fully supervised Random Forest!\n",
    "\n",
    "\n",
    "Another benefit is that the cluster labels now all align with the true class labels, because of the small amount of supervision, as the confusion matrix below will confirm (it should only be diagonal if the cluster labels are equal to the true class labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(sklearn.metrics.confusion_matrix(y_pred_gm2, y))\n",
    "plt.title('GM (semi-supervised)')\n",
    "\n",
    "plt.matshow(sklearn.metrics.confusion_matrix(y_pred_rf, y))\n",
    "plt.title('RF (supervised)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Semi-supervised Gaussian Mixture')\n",
    "ipywidgets.interact(lambda cluster: plot_faces_class(y_pred_gm2, cluster), cluster=(0,40-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes this week's exercises!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
