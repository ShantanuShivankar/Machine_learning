{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RO47002 Machine Learning for Robotics\n",
    "* (c) TU Delft, 2020\n",
    "* Period: 2020-2021, Q1\n",
    "* Course homepage: https://brightspace.tudelft.nl/d2l/home/318952"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE`. Moreover, if you see an empty cell, please DO NOT delete it, instead run that cell as you would run all other cells. Please fill in your name(s) and other required details below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_NUMBER = \"\"\n",
    "STUDENT_NAME1 = \"\"\n",
    "STUDENT_NUMBER1 = \"\"\n",
    "STUDENT_NAME2 = \"\"\n",
    "STUDENT_NUMBER2 = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3f76d6a626db81c484191482b101edb",
     "grade": true,
     "grade_id": "cell-c35e4c8223095209",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Note: this block is a check that you have filled in the above information.\n",
    "# It will throw an AssertionError until all fields are filled\n",
    "assert(GROUP_NUMBER != \"\")\n",
    "assert(STUDENT_NAME1 != \"\")\n",
    "assert(STUDENT_NUMBER1 != \"\")\n",
    "assert(STUDENT_NAME2 != \"\")\n",
    "assert(STUDENT_NUMBER2 != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General announcements\n",
    "\n",
    "* Do *not* share your solutions, and do *not* copy solutions from others. By submitting your solutions, you claim that you and your lab partner alone are responsible for this code.\n",
    "\n",
    "* Do *not* email questions directly, since we want to provide everybody with the same information and avoid repeating the same answers. Instead, please post your questions regarding this assignment in the correct support forum on Brightspace, this way everybody can benefit from the response. If you do have a particular question that you want to ask directly, please use the scheduled practicum hours to ask a TA.\n",
    "\n",
    "* There is a strict deadline for each assignment. Students are responsible to ensure that they have uploaded their work in time. So, please double check that your upload succeeded to the Brightspace and avoid any late penalties.\n",
    "\n",
    "* This [Jupyter notebook](https://jupyter.org/) uses `nbgrader` to help us with automated tests. `nbgrader` will make various cells in this notebook \"uneditable\" or \"unremovable\" and gives them a special id in the cell metadata. This way, when we run our checks, the system will check the existence of the cell ids and verify the number of points and which checks must be run. While there are ways that you can edit the metadata and work around the restrictions to delete or modify these special cells, you should not do that since then our nbgrader backend will not be able to parse your notebook and give you points for the assignment. You are free to add additional cells, but if you find a cell that you cannot modify or remove, please know that this is on purpose.\n",
    "\n",
    "* This notebook will have in various places a line that throws a `NotImplementedError` exception. These are locations where the assignment requires you to adapt the code! These lines are just there as a reminder for youthat you have not yet adapted that particular piece of code, especially when you execute all the cells. Once your solution code replaced these lines, it should accordingly *not* throw any exceptions anymore.\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# week 5\n",
    "\n",
    "* Topic: Dimensionality reduction, clustering\n",
    "* Year: 2020-2021\n",
    "* Book chapters: 8, 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This lab assignment consists of two parts.\n",
    "\n",
    "**This is Part 1 - Dimensionality Reduction**\n",
    "\n",
    "We mostly focus on Principal Component Analysis (PCA)\n",
    "\n",
    "* Understainding PCA transformations using on a 3D toy dataset\n",
    "* Implementing PCA project and reconstruction yourself\n",
    "* Using PCA for data compression\n",
    "* Using PCA on a real Pedestrian image dataset, and computing the \"Eigen-Pedestrians\"\n",
    "\n",
    "Part 2 will focus on clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "#%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on 3D Toy Data\n",
    "\n",
    "We will first create a small 3D \"toy\" dataset to investigate PCA as a dimensionality reduction technique.\n",
    "We'll  treat this data just as samples from some distribution over the 3D feaure space.\n",
    "\n",
    "For now, we will not consider that different samples have different class labels.\n",
    "There could be various reasons why we would want to consider PCA to consider such a data distribution. For instance:\n",
    "- these could all be samples from the same class and we want to understand how to model the class-conditional distribution in a Bayesian classifier\n",
    "- we might know that these will be samples from mulitple classes, but we just do not know the class labels yet (maybe these still need to be annotated) but sill wish to compress the dataset size\n",
    "- we might want to create a 2D plot of the samples, so we can get an intuition of the data in the higher dimensional space. E.g. maybe we can already determine if the classes are easily separable or not, and how their data is distributed (e.g. is the distribution skewed, symmetric, uncorrelated, etc.). A word of caution though: if they classes are easily separable in the 2D plot after a linear projection, they'll also be seperable by a linear classifier in the original higher dimensional space; However if they are not easily separable in 2D space, they could be seperable in a 2+ dimensional space, so this doesn't give a definitive answer.\n",
    "\n",
    "Our goal here is to use understand how PCA preserves the variance of the data by projecting to a lower dimensional space, and how we can perform the inverse projection from this projection back to the original 3D space to reconstruct the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and explore the 3D toy dataset\n",
    "\n",
    "The block below create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "mask = (y != 0)\n",
    "X = X[mask,:]\n",
    "y = y[mask]\n",
    "\n",
    "# make data a bit more interesting\n",
    "X = X[:,:3] # only use 3 dimensions\n",
    "X[:,1] *= 0.3 # scale down 2nd dimension\n",
    "X[:,0] += X[:,1] * 0.8\n",
    "X[:,2] *= 1.5\n",
    "\n",
    "y = (y == 2).astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could determine which features are correlated, and what features contain most of the variance in the data? One way is to compute and report some statistics on the features.\n",
    "\n",
    "Use the code block below the compute the variance of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ca2b8857aba5484f6c244c64fa02b14",
     "grade": false,
     "grade_id": "cell-9ed545710c9ad8e4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compute the variance of each feature, and print them to the output for instance.\n",
    "# You'll use the result to answer the question in the next code block.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your calculations, order the 3 features from most to least variance.\n",
    "\n",
    "Use the variable `FEATURE_ORDER_MOST_TO_LEAST_VARIANCE` to give your answer by listing the feature dimensions (0, 1, 2) in order of decreasing variance.\n",
    "For instance, if you answer that feature 0 has most variance, feature 1 the second most, and feature 2 the least variance, answer: `FEATURE_ORDER_MOST_TO_LEAST_VARIANCE = [0, 1, 2]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d6a3ff6cbc6bf20d15d67f7bdfe032e5",
     "grade": false,
     "grade_id": "cell-8e4433e32e4b2fd4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# replace the -1s here and put the numbers 0, 1, 2 in the right order\n",
    "FEATURE_ORDER_MOST_TO_LEAST_VARIANCE = [-1, -1, -1]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9207992a33142222534f724985823aac",
     "grade": true,
     "grade_id": "cell-699de6a1d97725b5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# ensure all numbers 0, 1, 2 occur once in your answer\n",
    "assert(len(FEATURE_ORDER_MOST_TO_LEAST_VARIANCE) == 3)\n",
    "assert(np.all(np.bincount(FEATURE_ORDER_MOST_TO_LEAST_VARIANCE) == [1,1,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics are useful, but don't show the structure of the samples directly.\n",
    "\n",
    "Since this is a 3D dataset, we can also try to visualize all 3 features at once in a 3D plot\n",
    "which can be rotated around to better understandin the underlying structure.\n",
    "The provided code below sets up the [3D plotting interface of matplotlib](https://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html), and uses the ipython widgets to allow you to adapt the viewing angle and see the data from different perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "def make_3d_plot_axes_equal(ax):\n",
    "    \"\"\" Utility function to make axes equally scaled for 3D plots in matplotlib.\n",
    "        Note that for 2D plots we can simply use ax.axes('equal'),\n",
    "        but unfortunately this doesn't work for 3D plots, so we use this utility function.\n",
    "        \n",
    "        Inspired by: https://stackoverflow.com/a/31364297\n",
    "    \"\"\" \n",
    "    \n",
    "    ax_limits = np.array([ax.get_xlim3d(), ax.get_ylim3d(), ax.get_zlim3d()]).T\n",
    "    \n",
    "    m = ax_limits.mean(axis=0)\n",
    "    max_range = (ax_limits - m).max();\n",
    "    \n",
    "    ax.set_xlim(m[0] - max_range, m[0] + max_range)\n",
    "    ax.set_ylim(m[1] - max_range, m[1] + max_range)\n",
    "    ax.set_zlim(m[2] - max_range, m[2] + max_range)\n",
    "    \n",
    "\n",
    "def plot_3d_data(X, view_angle1, view_angle2, label_name='dim'):\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    ax.view_init(view_angle1, view_angle2)\n",
    "    \n",
    "    ax.scatter(X[:,0], X[:,1], X[:,2], s=5., alpha=0.7)\n",
    "\n",
    "    plt.xlabel(label_name+' 0')\n",
    "    plt.ylabel(label_name+' 1')\n",
    "    ax.zaxis.set_label_text(label_name+' 2') # no plt.zlabel() :-/\n",
    "\n",
    "    # ensure 3D plot has equally scaled axes\n",
    "    make_3d_plot_axes_equal(ax)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# make rotatable 3D plot with standard plotting tools\n",
    "ipywidgets.interactive(\n",
    "    lambda view_angle1, view_angle2: plot_3d_data(X, view_angle1, view_angle2, label_name='feature'),\n",
    "    view_angle1=(0, 90),\n",
    "    view_angle2=(0, 360)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3D plot allows us to get a good feeling of how the data is distributed in the space, but it can be hard to really read off particular feature values for any sample.\n",
    "Alternatively, we could have projected the 3D to a 2D plane by only plotting 2 feature dimensions at once, ignoring the third. With 3 features, there are 3 possible feature combinations to plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_on_axes(X, feat_hor, feat_ver):\n",
    "    plt.plot(X[:, feat_hor], X[:, feat_ver], '.')\n",
    "    plt.xlabel(f'feature {feat_hor}')\n",
    "    plt.ylabel(f'feature {feat_ver}')\n",
    "    plt.axis('equal')\n",
    "    plt.grid('on')\n",
    "\n",
    "def plot_axis_combinations(X):\n",
    "    plt.subplot(1,3,1)\n",
    "    plot_data_on_axes(X, feat_hor=0, feat_ver=1)\n",
    "    \n",
    "    plt.subplot(1,3,2)\n",
    "    plot_data_on_axes(X, feat_hor=0, feat_ver=2)\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plot_data_on_axes(X, feat_hor=1, feat_ver=2)\n",
    "    \n",
    "plt.figure(figsize=(14,4))\n",
    "plot_axis_combinations(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Based on these plots, which of these feature pairs are most strongly correlated?\n",
    "\n",
    "Answer in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a0d9f9465e74938aa72086f371d3d2f",
     "grade": false,
     "grade_id": "cell-19f02c41cbbaa8fb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# replace the -1s here by the indices (0, 1 or 2) of the two strongest correlated features\n",
    "#   NOTE: order doesn't matter for this answer\n",
    "MOST_STRONLGY_CORRELATED_FEATURE_PAIR = (-1, -1)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8477ffa73a1f9a26733d7b58199f16c",
     "grade": true,
     "grade_id": "cell-4ed52fb4030b6979",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(len(MOST_STRONLGY_CORRELATED_FEATURE_PAIR) == 2)\n",
    "assert(MOST_STRONLGY_CORRELATED_FEATURE_PAIR[0] in (0,1,2))\n",
    "assert(MOST_STRONLGY_CORRELATED_FEATURE_PAIR[1] in (0,1,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the data with Principal Component Analysis\n",
    "\n",
    "The 2D plots give a better picture of the statstical relation between feature pairs, but all of them give you an incomplete picture because they ignore some aspects of the data. With higher dimensional data, there are many more possible feature combinations to explore. And, we can't plot all features in an interactive plot for more the 3 dimensions either.\n",
    "\n",
    "PCA allows us to find a linear transformation of the data, i.e. by performing a PCA projection we create a  transformed dataset where each feature is a linear combination of the original features.\n",
    "In the projected representation, the first dimension will capture most of the data variance, and the last dimension the least amount of variance.\n",
    "\n",
    "By only keeping the first few PCA dimensions, we can thus reduce high dimensional data to a smaller number of dimensions which still contain as much variations in the data as possibly could be kept with a linear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sklearn's PCA implementation to fit and transform the data $X$.\n",
    "Fow now, just use the default PCA options, don't set any keywords in the constructor.\n",
    "\n",
    "Remember that most of the intuition and basic usage of sklearn's PCA have been explained in Chapter 8 of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9b9d30fe608fdf5c8b43019988724ce",
     "grade": false,
     "grade_id": "cell-dac37007b8d673f6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_pca = None # store the result of the PCA projection of X in this variable\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7a81a1bada65f5719ec408a7b3c9cfc",
     "grade": true,
     "grade_id": "cell-84950c6c8a45795e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(X_pca.shape == (100, 3)) # 100 samples, 3 dimensions after projection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did with the original data features, determine again how much variance there is in each of the 3 dimensions after PCA transformation, and order these dimensions from most to least variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6589d5a9f6b763dfdf88c8abef0bc4a1",
     "grade": false,
     "grade_id": "cell-43e4f68273047489",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# replace the -1s here and put the numbers 0, 1, 2 in the right order\n",
    "PCA_PROJ_ORDER_MOST_TO_LEAST_VARIANCE = [-1, -1, -1]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d46ec60ff94b0df1782befc238ead896",
     "grade": true,
     "grade_id": "cell-252d2593b60dc1e5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# ensure all numbers 0, 1, 2 occur once in your answer\n",
    "assert(len(PCA_PROJ_ORDER_MOST_TO_LEAST_VARIANCE) == 3)\n",
    "assert(np.all(np.bincount(PCA_PROJ_ORDER_MOST_TO_LEAST_VARIANCE) == [1,1,1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data after the PCA project still has 3 dimensions. Therefore, we can again visualize the data using 3D or 2D plots, like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "plot_axis_combinations(X_pca)\n",
    "\n",
    "# Note: using X_pca here\n",
    "ipywidgets.interactive(\n",
    "    lambda view_angle1, view_angle2: plot_3d_data(X_pca, view_angle1, view_angle2, label_name='PCA proj.'),\n",
    "    view_angle1=(0, 90),\n",
    "    view_angle2=(0, 360)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Notice how the order and variance of each dimension after PCA projection is different from those of the original 3 feature dimensions (compare the data statistics here to those of the original data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the PCA components\n",
    "\n",
    "The PCA object has computed two important statistics,\n",
    "the `mean` of the data in the feature space,\n",
    "and the `principal components`.\n",
    "The principal components define a new *orthonormal basis* in the original feature space,\n",
    "located around the mean of the data.\n",
    "An orthonormal basis means that the basis vectors are\n",
    "1. all perpendicular to eachother\n",
    "2. all have unit length\n",
    "\n",
    "We can therefore regard it as a translation of the original feature, to subtract the mean, plus a rotation (and possibly with mirroring of some axes, which would just swap the sign in one of the dimensions).\n",
    "\n",
    "Let's visualize the mean and principal components in the original feature space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_pca_components(pca, X, view_angle1, view_angle2, label_name='dim'):\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # set view angle\n",
    "    ax.view_init(view_angle1, view_angle2)\n",
    "\n",
    "    # plot data\n",
    "    ax.scatter(X[:,0], X[:,1], X[:,2], s=5., alpha=0.3)\n",
    "\n",
    "    # get the data mean (a 3D vector) from the pca object, and plot it as star\n",
    "    m = pca.mean_\n",
    "    ax.plot((m[0],), (m[1],), (m[2],), 'k*', label='mean')\n",
    "    \n",
    "    # get each of the 3 pca components from the pca object, and plot it as a vector from the mean\n",
    "    for c in range(pca.components_.shape[0]):\n",
    "        comp = pca.components_[c]\n",
    "        color = 'rgb'[c]\n",
    "        p1 = m + comp\n",
    "        ax.plot((m[0], p1[0]), (m[1], p1[1]), (m[2], p1[2]), color+'-', label=f'PCA comp. {c}')\n",
    "    \n",
    "    # give all the axes a nice name\n",
    "    plt.xlabel(label_name+' 0')\n",
    "    plt.ylabel(label_name+' 1')\n",
    "    ax.zaxis.set_label_text(label_name+' 2') # no plt.zlabel() :-/\n",
    "    \n",
    "    # ensure 3D plot has equally scaled axes\n",
    "    make_3d_plot_axes_equal(ax)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# make rotatable 3D plot with standard plotting tools\n",
    "ipywidgets.interactive(\n",
    "    lambda view_angle1, view_angle2: plot_3d_pca_components(pca, X, view_angle1, view_angle2, label_name='feature'),\n",
    "    view_angle1=(0, 90),\n",
    "    view_angle2=(0, 360)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement some tests to show that the computed PCA components (`pca.components_`) indeed form an orthonormal basis. That is\n",
    "\n",
    "1. complete the function `component_is_unit_length(component)` which only returns True if the given vector `component` has unit length, and returns False otherwise.\n",
    "\n",
    "2. complete the function `components_are_orthogonal(comp1, comp2)` which only returns True if the two given vectors `comp1` and `comp2` are perpendicular to each other (*Hint*: what should the dot product of two perpendicular vectors be?)\n",
    "\n",
    "We'll then apply your functions to all PCA components and pairs of components.\n",
    "Note that for these tests you always need to allow for some small tolerance for small deviations of the desired outout, e.g. in the order of 10^-3 . The calculations of the computer have finite precision, so computed the length vector of a unit vector might actually be 1.000001, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26e4f705792615f4d8d17ac44d7c4772",
     "grade": false,
     "grade_id": "cell-b509cb77060d5487",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def component_is_unit_length(component):\n",
    "    \"\"\" Test if the length of a given component is 1.0\n",
    "        NOTE: you can allow some tolerance to numeric imprecision,\n",
    "        e.g. you can return True if the length l is between (0.999) < l < (1.001)\n",
    "        \n",
    "        Input: component - a vector\n",
    "        Output: is_unit_length - a boolean (True or False)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return is_unit_length\n",
    "\n",
    "def components_are_orthogonal(comp1, comp2):\n",
    "    \"\"\" Test two components are orthogonal.\n",
    "        NOTE: As in component_is_unit_length(), some tolerance to numeric imprecision is allowed\n",
    "        \n",
    "        Input: comp1 - a vector\n",
    "        Input: comp2 - a vector\n",
    "        Output: are_orthogonal - a boolean (True or False)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return are_orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "523392313e11fc29496da098cf4a2678",
     "grade": true,
     "grade_id": "cell-a9a9d36be1ac3d3e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test if each of the 3 PCA components have unit length\n",
    "for j in range(3):\n",
    "    comp = pca.components_[j]\n",
    "    \n",
    "    # your function is called here\n",
    "    is_unit_length = component_is_unit_length(comp)\n",
    "    \n",
    "    assert(is_unit_length in (False, True))\n",
    "    print(f'PCA component {j} has unit length:', is_unit_length)\n",
    "\n",
    "# Test if each pair of PCA components are orthogonal\n",
    "for j in range(3):\n",
    "    for k in range(j+1, 3):\n",
    "        comp1 = pca.components_[j]\n",
    "        comp2 = pca.components_[k]\n",
    "        \n",
    "        # your function is called here\n",
    "        are_orthogonal = components_are_orthogonal(comp1, comp2)\n",
    "        \n",
    "        assert(are_orthogonal in (False, True))\n",
    "        print(f'PCA components {j} and {k} are orthogonal:', are_orthogonal)\n",
    "\n",
    "# Some additional checks to make sure these functions perform correctly\n",
    "\n",
    "# test unit-length and non-unit-length components\n",
    "comp_x = np.array([1.0, 0.0, 0.0])\n",
    "comp_y = np.array([0.0, 1.0, 0.0])\n",
    "assert(components_are_orthogonal(comp_x, comp_y) == True)\n",
    "assert(components_are_orthogonal(comp_y*3, comp_x*-2) == True)\n",
    "# no non-zero vector is orthogonal to itself\n",
    "assert(components_are_orthogonal(comp_x, comp_x) == False)\n",
    "assert(components_are_orthogonal(comp_y, comp_y) == False)\n",
    "\n",
    "assert(component_is_unit_length(comp_x) == True)\n",
    "assert(component_is_unit_length(-comp_y) == True)\n",
    "assert(component_is_unit_length(comp_x*0.9999) == True) # within tolerance\n",
    "assert(component_is_unit_length(-comp_y*1.0001) == True) # within tolerance\n",
    "assert(component_is_unit_length(comp_x*0.99) == False) # outside tolerance\n",
    "assert(component_is_unit_length(comp_y*1.01) == False) # outside tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projecting to lower dimensional spaces\n",
    "\n",
    "To linearly project the $M$-dimensional data to only $D$-dimensions, i.e. transform $N \\times M$ matrix $X$ to a $N \\times D$ matrix $X_{pca}$, while maintaining as much variance in the data as possible.\n",
    "Construct below the following projections:\n",
    "\n",
    "* `X_pca2` the 2-dimensional PCA embedding of X\n",
    "* `X_pca1` the 1-dimensional PCA embedding of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8459b96f6706bb1c15518588a2f7c8f",
     "grade": false,
     "grade_id": "cell-f0136c384c01c6a6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_pca2 = None\n",
    "X_pca1 = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(X_pca2.shape, X_pca1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d06abd962774b403bfb41697d05a38c",
     "grade": true,
     "grade_id": "cell-ae2b961666c42f92",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(X_pca2.shape == (100,2))\n",
    "assert(X_pca1.shape == (100,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "#plot_axis_combinations(X_pca2_)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(X_pca2[:,0], X_pca2[:,1], '.')\n",
    "plt.title('2D projection')\n",
    "plt.axis('equal')\n",
    "plt.grid('on')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(X_pca1, np.zeros(X_pca1.shape), '.')\n",
    "plt.title('1D projection')\n",
    "plt.axis('equal')\n",
    "plt.grid('on')\n",
    "plt.yticks([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back-projecting to the original space\n",
    "\n",
    "Finally, let's try to reconstruct the data from the PCA projects back in the original $M$-dimensional feature space:\n",
    "\n",
    "* `X_recon1` should be the back projection from 1-D representation `X_pca1`\n",
    "* `X_recon2` should be the back projection from 1-D representation `X_pca2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc44045002367d0ecce16e0e58fff7ca",
     "grade": false,
     "grade_id": "cell-8f6a5252ac9fb291",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_recon1 = None # let this store the 3D reconstruciton after first projecting to a 1D PCA space\n",
    "X_recon2 = None # let this store the 3D reconstruciton after first projecting to a 2D PCA space\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e568923c71174475cc150218ca24f14e",
     "grade": true,
     "grade_id": "cell-e751a58466d70975",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(X_recon1.shape == X.shape)\n",
    "assert(X_recon2.shape == X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reuse our earlier plotting functions to show the reconstructed data as a 3D plot,\n",
    "including the use PCA components.\n",
    "\n",
    "Note that after back projecting, the data is again 3D, but intrinsically low dimensional as all data points now either lie on a 2D plane or 1D line in the 3D feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reconstruction from 2D PCA')\n",
    "# make rotatable 3D plot with standard plotting tools\n",
    "ipywidgets.interactive(\n",
    "    lambda view_angle1, view_angle2: plot_3d_pca_components(pca2, X_recon2, view_angle1, view_angle2, label_name='feature'),\n",
    "    view_angle1=(0, 90),\n",
    "    view_angle2=(0, 360)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reconstruction from 1D PCA')\n",
    "\n",
    "# make rotatable 3D plot with standard plotting tools\n",
    "ipywidgets.interactive(\n",
    "    lambda view_angle1, view_angle2: plot_3d_pca_components(pca1, X_recon1, view_angle1, view_angle2, label_name='feature'),\n",
    "    view_angle1=(0, 90),\n",
    "    view_angle2=(0, 360)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised dimensionality reduction\n",
    "\n",
    "\n",
    "While PCA is an unsupervised approach, since it doesn't consider the class labels,\n",
    "it is also possible to create a projection that does consider the class labels,\n",
    "for instance to maximize the kept variance between the classes, rather than the variance of the overall data.\n",
    "\n",
    "For intance, we could use LDA to learn such as projection. From Chapter 8:\n",
    "**Linear Discriminant Analysis (LDA)** *is a classification algorithm, but during training it learns the most discriminative axes between the classes, and these axes can then be used to define a hyperplane\n",
    "onto which to project the data. The benefit of this approach is that the projection\n",
    "will keep classes as far apart as possible, so LDA is a good technique to reduce\n",
    "dimensionality before running another classification algorithm such as an SVM\n",
    "classifier.*\n",
    "\n",
    "While the book doesn't go into detail on how the use LDA, the [sklearn interface for LDA](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis) is very similar to that of PCA for dimensionality reduction.\n",
    "Similar to PCA, you can set `n_components` in the LDA constructor, and perform fit() and transform() operations. Note that since this is supervised LDA, `n_components` can be at most equal to the number of classes - 1. E.g. in a 2 class problem, we can only find a `n_components=1`-dimensional subspace to best separate these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sklearn to create a supervised 1-dimensional LDA projection on the training data $X$, $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b2a1802460435664bd58ab36e4cde0a",
     "grade": false,
     "grade_id": "cell-f8e377c62b3c812f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_lda1 = None # X_lda1 will be the result of projecting X with LDA to a 1D space \n",
    "\n",
    "# Note that you might need to import the LDA implementation from sklearn first\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e29068e9160ad11fca2675717e643803",
     "grade": true,
     "grade_id": "cell-c3243dd635560d48",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(X_lda1.shape == (100,1))\n",
    "assert(X_pca1.shape == (100,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlims = (-4, 4)\n",
    "\n",
    "def plot_class_dist_histograms(X_1d, y):\n",
    "    nbins = 10\n",
    "    bins = np.linspace(xlims[0], xlims[1], nbins)\n",
    "    plt.hist(X_1d[y==0], bins, alpha=0.5, label='class 0')\n",
    "    plt.hist(X_1d[y==1], bins, alpha=0.5, label='class 1')\n",
    "    #plt.ylim([0., 1.0])\n",
    "    plt.ylabel('# samples')\n",
    "    plt.xlabel('projected 1D space')\n",
    "    plt.legend()\n",
    "\n",
    "plt.figure(figsize=(14,8))    \n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(X_pca1[y==0], np.zeros(X_pca1[y==0].shape), '.')\n",
    "plt.plot(X_pca1[y==1], np.zeros(X_pca1[y==1].shape), '.')\n",
    "plt.title('1D PCA projection')\n",
    "plt.axis('equal')\n",
    "plt.xlim(xlims)\n",
    "plt.grid('on')\n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(X_lda1[y==0], np.zeros(X_lda1[y==0].shape), '.')\n",
    "plt.plot(X_lda1[y==1], np.zeros(X_lda1[y==1].shape), '.')\n",
    "plt.title('1D LDA project')\n",
    "plt.axis('equal')\n",
    "plt.xlim(xlims)\n",
    "plt.grid('on')\n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plot_class_dist_histograms(X_pca1, y)\n",
    "plt.xlim(xlims)\n",
    "plt.grid('on')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plot_class_dist_histograms(X_lda1, y)\n",
    "plt.xlim(xlims)\n",
    "plt.grid('on')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the projected distributions in the plots above for PCA (left column) and LDA (right column). \n",
    "Note that the PCA projection doesn't scale the data to preserve the variance, it just defined a new coordinate system in the original space.\n",
    "The LDA projection on the other hand did scale the data, hence the total variance might appear larger than the PCA variance.\n",
    "\n",
    "**Q** Which dimensionality reduction method will have a lower Bayes error if you would use the projected data in a 1 dimensional classifier afterwards? Motivate your answer by observations that you make in the plots above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a99ce8be3f2193731798cd8a9877f3f",
     "grade": true,
     "grade_id": "cell-041c36bb405d3a77",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement your own PCA\n",
    "\n",
    "The math behind the standard PCA implementaion is pretty straightforward, and explained in book chapter 8.\n",
    "Let's verify that your implementation performs the same as sklearn's PCA implementation.\n",
    "\n",
    "Complete the following functions in the code blocks below:\n",
    "\n",
    "* `pca_fit(X, n_components)` to estimate the PCA transformation to a `n_component`-dimensional subspace\n",
    "* `pca_transform(m, components, X)` to apply the PCA transformation to data X\n",
    "* `pca_inverse_transform(mean, components, X_pca)` to perform the back-projection to the original feature space\n",
    "\n",
    "These functions will behave similar to sklearn's PCA fit(), transform() and inverse_transform() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6338fc0368cd129ed3c1f615ea3684fb",
     "grade": false,
     "grade_id": "cell-5316bbc7437b8422",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: don't use any sklearn functions in your own (re)-implementation. You can use np.linalg.svd though.\n",
    "\n",
    "def pca_fit(X, n_components):\n",
    "    \"\"\" Given an N x D input data matrix `X` containing N data samples in a D dimensional space,\n",
    "        Compute the parameters of the PCA projection, namely\n",
    "        - mean: the D-dimensional data mean, and\n",
    "        - components: a `n_components` x D matrix containing the first `n_components` PCA components\n",
    "        \n",
    "        Returns: mean, components\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return mean, components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b39c675fe1d58e2b52036551841a92b",
     "grade": true,
     "grade_id": "cell-17597d66bea52c34",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n_comp = 2\n",
    "\n",
    "# run your implementation\n",
    "mean, components = pca_fit(X, n_components=n_comp)\n",
    "\n",
    "assert( mean.shape == (3,) ) # mean should be 3D vector\n",
    "assert( components.shape == (n_comp, 3) ) # each of the n components should be a 3D vector\n",
    "\n",
    "# -- Comparison to Sklearn's PCA implementation --\n",
    "\n",
    "pca_ref = PCA(n_components=n_comp)\n",
    "pca_ref.fit(X)\n",
    "\n",
    "# First, we'll check if you have the same mean vector\n",
    "assert(np.all( pca_ref.mean_ == mean ))\n",
    "\n",
    "# Next, we'll check each of the PCA components in turn\n",
    "# Note that since the sign of a component is arbitray, so we should ensure that signs are aligned\n",
    "# before comparing the solutions of different PCA implementations\n",
    "\n",
    "def is_same_within_tolerance(a, b):\n",
    "    # test if two vectors are similar, within a small numerical error\n",
    "    return np.all(np.abs(a - b) < 1e-10)\n",
    "\n",
    "for c in range(n_comp):\n",
    "    ref_comp = pca_ref.components_[c]\n",
    "    your_comp = components[c]\n",
    "    \n",
    "    print(f'testing if components {c} is  similar ...')\n",
    "    print('  sklearn PCA:', ref_comp)\n",
    "    print('     your PCA:', your_comp)\n",
    "    \n",
    "    is_similar = is_same_within_tolerance(ref_comp, your_comp)\n",
    "    is_similar_mirrored = is_same_within_tolerance(ref_comp, -your_comp) # test with flipped sign\n",
    "    \n",
    "    if is_similar:\n",
    "        print('OK: components are the same')\n",
    "    elif is_similar_mirrored:\n",
    "        print('OK: components are the same [only sign is flipped]')\n",
    "    else:\n",
    "        print('ERROR: components are the different!')\n",
    "    print()\n",
    "    assert(is_similar or is_similar_mirrored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the transformation function that projects data X to the learned PCA space. Use standard numpy functions only, don't use any sklearn functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "155ad2f55f067e6c950a06bdc84f96c3",
     "grade": false,
     "grade_id": "cell-8001b22339508694",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pca_transform(mean, components, X):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return X_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a67950c15603d894c83b468373decbfb",
     "grade": true,
     "grade_id": "cell-c9f3db42ae939fc4",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_pca = pca_ref.transform(X)\n",
    "X_your_pca = pca_transform(mean, components, X)\n",
    "\n",
    "assert(X_pca.shape == X_your_pca.shape)\n",
    "\n",
    "# Let's create a plot of the resulting projections\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(X_your_pca[:,0], X_your_pca[:,1], '.')\n",
    "plt.title('your implementation')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(X_pca[:,0], X_pca[:,1], 'x')\n",
    "plt.title('sklearn implementation')\n",
    "plt.grid()\n",
    "\n",
    "# again, the signs of the projection might be flipped.\n",
    "#  to ensure that both implementaions use the same sign,\n",
    "#  we'll flip the sign if the sign of the first element is negative\n",
    "X_pca_ = X_pca.copy()\n",
    "X_your_pca_ = X_your_pca.copy()\n",
    "for c in range(n_comp):\n",
    "    if X_pca_[0,c] < 0: X_pca_[:,c] = -X_pca_[:,c]\n",
    "    if X_your_pca_[0,c] < 0: X_your_pca_[:,c] = -X_your_pca_[:,c]\n",
    "\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(X_your_pca_[:,0], X_your_pca_[:,1], '.', label='your implementation')\n",
    "plt.plot(X_pca_[:,0], X_pca_[:,1], 'x', label='sklearn implementation')\n",
    "plt.title('after aligning signs (plots should overlap)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Finally, let's run some asserts to check that the outcomes are indeed the same\n",
    "assert(np.all(np.abs( X_your_pca_ - X_pca_ ) < 1e-12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, implement the inverse transformation which projects the points from the embedded PCA space back to the original 3D feature space. Use again standard numpy functions only, don't use any sklearn functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64aac95e5b339a449fc40b8862eff097",
     "grade": false,
     "grade_id": "cell-1f1f662977a51e04",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pca_inverse_transform(mean, components, X_pca):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return X_reconstruct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8cbe605df1871c9f71542670b54b7679",
     "grade": true,
     "grade_id": "cell-24ff5c1c8d44b4d8",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your implementation\n",
    "X_your_reconstruct = pca_inverse_transform(mean, components, X_your_pca)\n",
    "\n",
    "# sklearn PCA reference\n",
    "X_ref_reconstruct = pca_ref.inverse_transform(X_pca)\n",
    "\n",
    "# solutions the same?\n",
    "assert( X_ref_reconstruct.shape == X_your_reconstruct.shape )\n",
    "assert( np.all( np.abs( X_ref_reconstruct - X_your_reconstruct ) < 1e-12 ) )\n",
    "\n",
    "# visually verify that your code has projected all points to a 2D plane in the original 3D feature space\n",
    "\n",
    "# Note: using X_your_reconstruct here\n",
    "ipywidgets.interactive(\n",
    "    lambda view_angle1, view_angle2: plot_3d_data(X_your_reconstruct, view_angle1, view_angle2, label_name='feat.'),\n",
    "    view_angle1=(0, 90),\n",
    "    view_angle2=(0, 360)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any given dataset, the number of possible PCA components is bounded by the number of samples and number of features.\n",
    "Express this straightforward relationship by completing the following function.\n",
    "\n",
    "If you don't know the answer, realize that the PCA components are defined by the number of non-zero eigenvalues of the data covariance matrix, e.g. the rank of the covariance matrix. Of course, you could also just experiment with sklearn's implementation and see what works, or look it up somewhere ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6bf2b9d5a4fc7c15c2b70b7be8196b7b",
     "grade": false,
     "grade_id": "cell-21e8646bff1e043e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def max_possible_PCA_components_for_given_dataset(num_samples, num_features):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return max_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b32a043b395c5bb1f35f1f4fe351bde",
     "grade": true,
     "grade_id": "cell-c5db4978a2e939f2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test same sample cases\n",
    "num_samples = 3\n",
    "num_features = 3\n",
    "n_comp = max_possible_PCA_components_for_given_dataset(num_samples, num_features)\n",
    "print(f'Maximimum number of PCA components for dataset with {num_samples} samples and {num_features} features: {n_comp}')\n",
    "\n",
    "num_samples = 1000\n",
    "num_features = 42\n",
    "n_comp = max_possible_PCA_components_for_given_dataset(num_samples, num_features)\n",
    "print(f'Maximimum number of PCA components for dataset with {num_samples} samples and {num_features} features: {n_comp}')\n",
    "\n",
    "num_samples = 300\n",
    "num_features = 1024\n",
    "n_comp = max_possible_PCA_components_for_given_dataset(num_samples, num_features)\n",
    "print(f'Maximimum number of PCA components for dataset with {num_samples} samples and {num_features} features: {n_comp}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigen-pedestrians\n",
    "\n",
    "Now that we have explored PCA with some toy data, let's apply it to some real world data.\n",
    "You are given a dataset containing 1500 gray-scale pedestrian image patches obtained from a driving vehicle.\n",
    "The $25 \\times 50$ pixel images have been reshaped to 1250-dimensional feature vectors.\n",
    "\n",
    "We start by loading the dataset from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "data_int = scipy.io.loadmat('data/ped_int.mat')\n",
    "\n",
    "X_ped = data_int['ped_int']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first explore the dataset a bit. As we have done in earlier assignments,\n",
    "it is possible to visualize the feature vectors back as  gray-level intensity images by resizing\n",
    "them to their original size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_HEIGHT = 50\n",
    "IM_WIDTH = 25\n",
    "\n",
    "# check we have correcly loaded the data\n",
    "assert(X_ped.shape[1] == IM_HEIGHT * IM_WIDTH)\n",
    "\n",
    "def feat_to_image(X):\n",
    "    return X.reshape(IM_WIDTH, IM_HEIGHT).T\n",
    "\n",
    "def plot_feat_as_image(X):\n",
    "    image = feat_to_image(X)\n",
    "    plt.imshow(image, cmap=mpl.cm.gray)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def plot_multiple_feats_as_images(X, num_cols=10, suptitle=None):\n",
    "    plt.figure(figsize=(16,2))\n",
    "    \n",
    "    for i in range(num_cols):\n",
    "        sample = X[i]\n",
    "        image = feat_to_image(sample)\n",
    "\n",
    "        plt.subplot(1,num_cols,i+1)\n",
    "        plt.imshow(image, cmap=mpl.cm.gray)\n",
    "        plt.title('%d' % i)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    if suptitle:\n",
    "        plt.suptitle(suptitle)\n",
    "        \n",
    "print('Number of training samples:', X_ped.shape[0])\n",
    "print('        Number of features:', X_ped.shape[1])\n",
    "\n",
    "plot_multiple_feats_as_images(X_ped, 20, suptitle='First 20 samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on image data\n",
    "\n",
    "In all these images there is a pedestrian in the center, with some variations in pose, viewpoint, clothing, lighting conditions, background, body size, etc.\n",
    "We can use PCA to capture the main modes of variation in these images,\n",
    "similar to what we have done in 3D toy example, but now using 1250 features.\n",
    "\n",
    "\n",
    "**Q**: How many PCA components could we maximimally compute for this pedestrian dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9fa51bb5304ebd040586199ea38adb24",
     "grade": false,
     "grade_id": "cell-6947ec38969cc12a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0eeda81af830d730d45f3a0e900450a6",
     "grade": true,
     "grade_id": "cell-cc3eea8b9fc19484",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('Maximimum number of PCA components for a pedestrian dataset:')\n",
    "print(MAX_PCA_COMPONENTS_FOR_PED_DATASET)\n",
    "\n",
    "assert(type(MAX_PCA_COMPONENTS_FOR_PED_DATASET) == int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's fit a 50-dimensional PCA transformation for this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7120841cf7887357d0b4b5510f88512d",
     "grade": false,
     "grade_id": "cell-e0daced1f99829d1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pca = None # store sklearn's PCA object here\n",
    "X_ped_pca = None # store the PCA projection of X_ped here\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a5ebb4ab5156b2ddde8b8c9343419bf",
     "grade": true,
     "grade_id": "cell-e5d7e36c3530920e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(not pca is None)\n",
    "assert(X_ped_pca.shape == (1500,50))\n",
    "sklearn.utils.validation.check_is_fitted(pca) # will throw error if not fitted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the features in X represent the intensity values of the individual pixels in the images,\n",
    "the 1250-dimensional mean feature vector contains the mean intensity value of each pixel over the whole dataset,\n",
    "and can thus also be visualized as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feat_as_image(pca.mean_)\n",
    "plt.title('Mean feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, the PCA components which represent the main axes of variation in the 1250-dimensional dataset can *also* be visualized as images.\n",
    "\n",
    "We'll refer to these images as the **\"Eigen-pedestrians\"**, similar to the concept of [Eigenfaces](https://en.wikipedia.org/wiki/Eigenface),\n",
    "as we can reconstruct pedestrian images from our target distribution very accuratly using only the mean image and a small number of these eigen-pedestrians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_feats_as_images(pca.components_, 20, suptitle='First 20 PCA components (\"Eigen-pedestrians\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we can recreate different pedestrian images by taking a linear combinations of these PCA components,\n",
    "and adding them to the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_show_pca_reconstruction(w0=0, w1=0., w2=0., w3=0., w4=0., w10=0., w18=0.):\n",
    "    # the indices of the PCA components in this function's arguments\n",
    "    idxs = np.array([0, 1, 2, 3, 4, 10, 18])\n",
    "    \n",
    "    # construct weight vector with given values\n",
    "    w = np.array([w0, w1, w2, w3, w4, w10, w18])\n",
    "    \n",
    "    x = pca.mean_ + w.dot(pca.components_[idxs])\n",
    "    \n",
    "    image = feat_to_image(x)\n",
    "    plt.imshow(image, cmap=mpl.cm.gray) # note. mpl.cm.binary inversts the colors\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "wrange = (-1600., 1600.)\n",
    "ipywidgets.interact(plot_show_pca_reconstruction, w0=wrange, w1=wrange, w2=wrange, w3=wrange, w4=wrange, w10=wrange, w18=wrange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression\n",
    "\n",
    "We will now try to see how we can compress the pedestrian images by using only a few Eigen-pedestrians.\n",
    "Complete the function below which takes the dataset, and a given number of Eigen-pedestrians (PCA components),\n",
    "and use it to \n",
    "\n",
    "1. first compress the data to `n_components` with PCA, and then.\n",
    "2. reconstruct the images from the compressed representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfbbd532a533cf21f3387e2cb4f1a907",
     "grade": false,
     "grade_id": "cell-874d6fe677771805",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compress_and_decompress_with_pca(X_ped, n_components):\n",
    "    \"\"\" Compress and decompress features in dataset X_ped\n",
    "        by first projecting the data to a PCA subspace with n_components,\n",
    "        and the reconstructing the data from this PCA embedding.\n",
    "        \n",
    "        Input: X_ped - N x M dataset\n",
    "        Input: n_components - integer, target number of components for compression\n",
    "        Returns: X_reconstruct - N x M with reconstructed data after compression\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return X_reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5357285e2f97b4d05e3d678183358d36",
     "grade": true,
     "grade_id": "cell-75f2845e27166e7e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_reconstruct = compress_and_decompress_with_pca(X_ped, n_components=20)\n",
    "\n",
    "# reconstruction should keep the dimensionality of the input\n",
    "assert(X_reconstruct.shape == X_ped.shape)\n",
    "\n",
    "# after compression with 0 components, all samples should just be equal to the mena\n",
    "#  so we should see no variance left in the data\n",
    "X_reconstruct = compress_and_decompress_with_pca(X_ped, n_components=0)\n",
    "assert(np.all( X_reconstruct.var(axis=0) < 1e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can below interactivaly visualize the result and see what happens if we increase step by step the number of components from 0 to 1, 2, 3, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_compression(n_components):\n",
    "    X_reconstruct = compress_and_decompress_with_pca(X_ped, n_components)\n",
    "\n",
    "    print(f'num components: {n_components}') \n",
    "    plot_multiple_feats_as_images(X_ped, suptitle='original data')\n",
    "    plot_multiple_feats_as_images(X_reconstruct, suptitle=f'reconstruction with {n_components} PCA components (+ mean)')\n",
    "\n",
    "ipywidgets.interact(plot_pca_compression, n_components=ipywidgets.IntSlider(0, min=0,max=1250, continuous_update=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that especially in the beginning with few components, adding an additional component has a large impact on the reconstruction quality, since these initial components still explain a lot of the variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** If we want to lossy compress a dataset $X$ with $N$ samples and a $M$-dimensional feature space, with only $D$ PCA components, how much storage space have we gained?\n",
    "\n",
    "To compute the compression rate $r$, \n",
    "we have to consider\n",
    "- how many numbers were needed to represent the original data set (i.e. the number of elements in the matrix X), and\n",
    "- how many numbers are needed to represent the compressed data\n",
    "- how many numbers are needed to store the \"compression information\", i.e. the relevant PCA parameters.\n",
    "\n",
    "In fact, you can use the following formula:\n",
    "$r = \\frac{N(X)}{N(X_{pca}) + m*N(comp) + N(mean)}$\n",
    "\n",
    "where:\n",
    "- $N(X)$ is the number of (floating point) numbers in the original data set $X$\n",
    "- $N(X_{pca})$ is the number of numbers in the PCA compressed data set $X_{pca}$\n",
    "- $N(comp)$ is the number of (floating point) numbers to represent a single PCA component\n",
    "- $N(mean)$ is the number of (floating point) numbers to represent the mean feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f73c2bc61b20210ae43a3f4be72caa1",
     "grade": false,
     "grade_id": "cell-6760c8addd6ef802",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_data_compression_rate(N, M, D):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return COMPRESSION_RATIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c88a2f25afba2724653ebc090322298c",
     "grade": true,
     "grade_id": "cell-9e5f8e6c8ffd86c4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "N = 1500\n",
    "M = 1024\n",
    "D = 50\n",
    "\n",
    "COMPRESSION_RATIO = compute_data_compression_rate(N, M, D)\n",
    "print(f'With {D} PCA components, {N} samples in a {M}-dimensional feature space')\n",
    "print('can be compressed  by a factor %.2fx' % COMPRESSION_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained variance\n",
    "\n",
    "Let's inspect how the number of PCA components affects the amount of variance in that data that the project keeps,\n",
    "and verify that the result reported in the PCA object coincides with what we observe in the data.\n",
    "\n",
    "Complete the two functions below\n",
    "\n",
    "**Note**: the total variance in the data is defined as the sum of the variance per feature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7a96818a2ebe2e84192b73da95823b2",
     "grade": false,
     "grade_id": "cell-19a5a0904d321916",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def total_variance_explained_in_pca_projection(pca):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return total_variance_ratio\n",
    "\n",
    "def total_variance_ratio_in_data(X, X_pca):\n",
    "    \"\"\" Compute the ratio of variance in the original data X\n",
    "        still present in compressed data X_pca:\n",
    "        \n",
    "           total-ratio = amount-variance-in-X_pca / amount-variance-in-X\n",
    "        \n",
    "        Note that the amount of variance of a dataset X is \n",
    "        defined as the sum of the variance of each feature in that data.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return total_variance_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b75a859f29de2ecd8b63ba4dba7b5148",
     "grade": true,
     "grade_id": "cell-a0f009cecf41f588",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "variance_explained_predicted = total_variance_explained_in_pca_projection(pca)\n",
    "variance_explained_observed = total_variance_ratio_in_data(X_ped, X_ped_pca)\n",
    "\n",
    "print('Ratio of kept variance according to PCA implementation: %.2f' % (variance_explained_predicted*100.))\n",
    "print('                Observed rato of kept variance in data: %.2f' % (variance_explained_observed*100.))\n",
    "\n",
    "# these numbers should be identical\n",
    "assert(abs(variance_explained_observed - variance_explained_observed) < 1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What is the minimum number of PCA components if we want to keep at least 80% of the variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a224192a71e09b5be1c58776cccf7cd7",
     "grade": false,
     "grade_id": "cell-050f80b6fc1038de",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_COMPONENTS_NEEDED = -1\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4bfe05da50d82a7d4983cb6fcd6f40ab",
     "grade": true,
     "grade_id": "cell-6c923deab5862fab",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('Your answer: at least', NUMBER_OF_COMPONENTS_NEEDED, 'are needed to explain 80% of the variance in the data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes part 1 on dimensionality reduction and PCA, please continue with part 2 on clustering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
