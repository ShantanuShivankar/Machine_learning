{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Evaluation\n",
    "Machine Learning 2019/2020 <br>\n",
    "Ruben Wiersma, David Tax and Jordi Smit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHAT** This nonmandatory lab consists of several open programming and insight exercises/questions on evaluation estimation.\n",
    "\n",
    "**WHY** The exercises are meant to explore evaluation methods.\n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a friend. If you want to skip right to questions and exercises, find the $\\rightarrow$ symbol. Use [Mattermost][1] to discuss questions with your peers. For additional questions and feedback please consult the TA's during the lab session. \n",
    "\n",
    "[1]: https://mattermost.ewi.tudelft.nl/ml/channels/town-square\n",
    "$\\newcommand{\\q}[1]{\\rightarrow \\textbf{Question #1}}$\n",
    "$\\newcommand{\\ex}[1]{\\rightarrow \\textbf{Exercise #1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating classifiers\n",
    "This is an additional assignment whereby you will learn to use the [scikit-learn](https://scikit-learn.org/stable/) library. We will do this by searching for the best classifier for the MNIST dataset. We will start by comparing the  parametric (logistic regression) and non-parametric (k-nearest neighbours) classifier but you are free to try different classifiers after your done.  Trying out different  classifiers is rather easy since they all follow the same API.\n",
    "\n",
    " **In contrast to the previous weeks, the structure of this notebook in this week is open. This notebook has been designed as an open ended data science experiment. That is why may find that not everything is 100% specified  and most of the code is not given. Like in real life you have to make some choices on what to use and research how and what to use in the documentation on your own.** \n",
    "\n",
    "This notebook consists of three parts: design, implementation, results & analysis. \n",
    "We provide you with the design of the experiment and you have to implement it and analyse the results.\n",
    "\n",
    "__To make the most of this exercise, follow the following guidelines:__\n",
    "* Explain and analyse all results. Try to do this in markdown blocks in your notebook.\n",
    "* Make your notebook easy to read. When you are finished take your time to review it!\n",
    "* You do not want to repeat the same chunks of code multiply times. If your need to do so, write a function. \n",
    "* The implementation part of this assignment needs careful design before you start coding. You could start by writing pseudocode.\n",
    "* In this exercise the insights are important. Do not hide them somewhere in the comments in the implementation, but put them in the Analysis part.\n",
    "* A plot should have a title and axes labels.\n",
    "* You may find that not everything is 100% specified in this assignment. That is correct! Like in real life you probably have to make some choices. Motivate your choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using scikit-learn\n",
    "\n",
    "In this exercise, you can use scikit-learn implementations for k-nn and logistic regression. An example of how to use logistic regression from scikit-learn library is provided for you reference.\n",
    "\n",
    "Consult the documentation to find out what these classifiers can do:\n",
    "- [Logistic Regression documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "- [K-NN documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9733333333333334\n",
      "0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shantanu/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Import sklearn modules\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load in a dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Create a classifier object that can be called to predict datasets\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(X, y)\n",
    "knn = KNeighborsClassifier(n_neighbors=2, algorithm='ball_tree').fit(X, y)\n",
    "\n",
    "# Predict class labels for X\n",
    "clf.predict(X[:, :])\n",
    "knn.predict(X[:, :])\n",
    "\n",
    "# Returns the mean accuracy on the given test data and labels.\n",
    "print(clf.score(X, y))\n",
    "print(knn.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not have to keep the order of this design and are allowed to alter it if you are confident.\n",
    "1. Load the provided train and test set of MNIST dataset and preform pre processing.\n",
    "2. Do  a grid search for logistic regression and k-nn to find the optimal parameters.\n",
    "3. Train logistic regression and k-nn using optimized parameters.\n",
    "4. Compare performance on the test set for two classifiers.\n",
    "5. Discuss your results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Hint: you might need one of these functions\n",
    "%pylab inline\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "K_FOLDS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the data and preprocessing\n",
    "In this part you will load the MNIST dataset (`digits = load_digits()`). After you have loaded the dataset carefully examine the features and ask your self the following questions:\n",
    "\n",
    " - What is the shape of the `X` and `Y` data and do I need to reformat them?  X dimension (1797, 8, 8), Y dimension (1797,) \n",
    " - What are the min, max and mean values of these features? 0 16 somewhere in between\n",
    " - Will scaling the features help based on these values?\n",
    " - What are the effects and how can the following sklearn functions help [`StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and [`MinMaxScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)?\n",
    " Standard sacler scales the data using 0 mean and var=1. Min max scales the data in the given range\n",
    " \n",
    "After you have loaded and preprocesed the data you should split it into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797,)\n",
      "[0 1 2 ... 8 9 8]\n",
      "The numbers shown are: [[0 1 2 3 4]\n",
      " [5 6 7 8 9]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Student\n",
    "# Import the load function for the dataset\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Load the digits with 10 classes (0 - 9)\n",
    "digits = load_digits(n_class=10)\n",
    "\n",
    "# Plot the numbers in two rows\n",
    "firstrow = np.hstack(digits.images[:5,:,:])\n",
    "print(np.shape(digits.target))\n",
    "print(digits.target)\n",
    "white_line = np.ones((1, 40)) * 16\n",
    "secondrow = np.hstack(digits.images[5:10,:,:])\n",
    "\n",
    "# Show both rows at the same time\n",
    "plt.gray()\n",
    "plt.axis('off')\n",
    "plt.imshow(np.vstack((firstrow, white_line, secondrow)))\n",
    "\n",
    "print(\"The numbers shown are:\", np.vstack((digits.target[:5], digits.target[5:10])))\n",
    "\n",
    "#X_train, X_validation, y_train, y_validation = train_test_split(digits[0,:,:],train_size = 0.7,random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. K-Fold validation\n",
    "Consult the documentation on how cross validation works in scikit-learn (important function:[GridSearchCV()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)). Then continue by performing 10-Fold cross-validation for both the Logistic regression and K-NN classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a Logistic regression\n",
    "Use 10-fold cross validation and a grid search to optimize the performance for the *regularization parameter* (Hint: look at the `C` parameter). Keep all other parameters constant and the default values by specifying  only a single value for them. You can find the default values at  the [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) API documentation. Once you are done with the K-Fold validation, try to find out what are the best parameters and what score they achieved.\n",
    "\n",
    "**Hint: You can use `best_params_` and `best_score_` after fitting the grid search classifier to get the best parameters found and the corresponding score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b K-NN\n",
    "Use 10-fold cross validation and a grid search to optimize the performance for the *K parameter*. Keep all other parameters constant and the default values by specifying  only a single value for them. You can find the default values at  the [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) API documentation. Once you are done with the K-Fold validation, try to find out what are the best parameters and what score they achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train using the optimized parameters.\n",
    "Now lets train both the `LogisticRegression` and the `KNeighborsClassifier` using the optimized parameters. After you have trained them both, do the following things for both classifiers:\n",
    "\n",
    " - Calculate the average cross validation error over the K-folds;\n",
    " - Calculate the standard deviation error over the K-folds;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compare performance on the test set for two classifiers:\n",
    "* produce the classification report for both classifiers, consisting of precision, recall, f1-score. Explain and analyse the results.\n",
    "* print confusion matrix for both classifiers and compare whether they missclassify the same  classes. Explain and analyse the results.\n",
    "* Performance curves\n",
    "* [Learning curves](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html#sklearn.model_selection.learning_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.a Compare the classification report for both classifiers\n",
    "You can use the [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) function to do this. How do the precision, recall, f1-score compare between the 2 classifiers? Explain and analyse the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.b Compare the confusion matrix for both classifiers\n",
    "You can use the [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) function to do this. Than compare whether they missclassify the same classes. Explain and analyse the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.c What is the effect of more training data\n",
    "You can use the [learning_curve](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html) function to do this. Than compare how the mean accuracy of the test data is affect over the K-folds is affect by more training data for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Discuss your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which classifier preforms better on this dataset and under which conditions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Bonus\n",
    "* Tune more parameters\n",
    "* Add additional classifiers (NN, Naive Bayes, decision tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
